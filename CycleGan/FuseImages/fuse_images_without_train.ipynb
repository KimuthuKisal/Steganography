{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "print(\"Cuda available :\", torch.cuda.is_available())\n",
    "print(\"cuda version : \", torch.backends.cudnn.version())\n",
    "print(\"No of GPU : \", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"Device {i} Name: \", torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image inputs and preprocess\n",
    "def preprocess_images(image_path1:str, image_path2:str, height_width:int=256) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((height_width, height_width)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image1 = Image.open(image_path1).convert(\"RGB\")\n",
    "    image2 = Image.open(image_path2).convert(\"RGB\")\n",
    "    print(\"Shape of Image 1 : \", image1.size)\n",
    "    print(\"Shape of Image 2 : \", image2.size)\n",
    "    image1 = transform(image1).unsqueeze(0).to('cuda')\n",
    "    image2 = transform(image2).unsqueeze(0).to('cuda')\n",
    "    print(\"Shape of Reshaped Image 1 : \", image1.shape)\n",
    "    print(\"Shape of Reshaped Image 2 : \", image2.shape)\n",
    "    return image1, image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image:torch.Tensor) -> None:\n",
    "    image_np = image.squeeze().cpu().detach().numpy()\n",
    "    image_np = image_np.transpose(1, 2, 0)  # (C, W, H) -> (W, H, C)\n",
    "    plt.imshow(image_np)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path1 = './original_images/fuse1.JPEG'  \n",
    "image_path2 = './original_images/fuse2.JPEG'  \n",
    "img1, img2 = preprocess_images(image_path1, image_path2)\n",
    "show_image(img1)\n",
    "show_image(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FuseModel, self).__init__()\n",
    "\n",
    "        # Downsampling\n",
    "        self.down_reflection_pad = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=1, padding=0),  \n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down_resnet_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        # Upsampling\n",
    "        self.up_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(256),  \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up_deconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up_deconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up_resnet_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.up_reflection_pad = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3), \n",
    "            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=7, stride=1, padding=0),  # 7x7 Conv with no additional padding\n",
    "            nn.InstanceNorm2d(3),  \n",
    "            nn.Tanh() \n",
    "        )\n",
    "\n",
    "    def forward(self, x1:torch.Tensor, x2:torch.Tensor) -> torch.Tensor:\n",
    "        print(\"Input shape : \", x1.shape, \"and\", x2.shape)\n",
    "\n",
    "        # Downsampling\n",
    "        x1 = self.down_reflection_pad(x1)  \n",
    "        x2 = self.down_reflection_pad(x2)  \n",
    "        # print(\"Shape after Down 1 layer : \", x1.shape, \"and\", x2.shape)\n",
    "        x1 = self.down_conv1(x1) \n",
    "        x2 = self.down_conv1(x2) \n",
    "        # print(\"Shape after Down 2 layer : \", x1.shape, \"and\", x2.shape)          \n",
    "        x1 = self.down_conv2(x1)  \n",
    "        x2 = self.down_conv2(x2)  \n",
    "        # print(\"Shape after Down 3 layer : \", x1.shape, \"and\", x2.shape)         \n",
    "        for i in range(9):\n",
    "            x1 = self.down_resnet_block1(x1)  \n",
    "            x2 = self.down_resnet_block1(x2)  \n",
    "        # print(\"Shape after Down resnet layers : \", x1.shape, \"and\", x2.shape)\n",
    "        \n",
    "        \n",
    "        # Concatenation\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        # print(\"Shape after Concatenation : \", x.shape)\n",
    "\n",
    "        # Upsampling\n",
    "        x = self.up_conv1(x)\n",
    "        # print(\"Shape after Up 1 layer : \", x.shape)\n",
    "        for i in range(1):\n",
    "            x = self.up_resnet_block1(x)\n",
    "        for i in range(9):\n",
    "            x = self.up_resnet_block1(x)\n",
    "        for i in range(1):\n",
    "            x = self.up_resnet_block1(x)\n",
    "        for i in range(9):\n",
    "            x = self.up_resnet_block1(x)\n",
    "        # print(\"Shape after Up resnet layers : \", x.shape)\n",
    "        x = self.up_deconv1(x)\n",
    "        # print(\"Shape after Up 2 layer : \", x.shape)    \n",
    "        x = self.up_deconv2(x)\n",
    "        # print(\"Shape after Up 3 layer : \", x.shape)     \n",
    "        x = self.up_reflection_pad(x)\n",
    "        # print(\"===Output shape : \", x.shape)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# fuse_model = FuseModel().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fused_image = fuse_model(img1, img2)\n",
    "# detached_fused_image = fused_image.squeeze().cpu().detach().numpy()\n",
    "# detached_fused_image = detached_fused_image.transpose(1, 2, 0)\n",
    "# print(detached_fused_image.shape)\n",
    "# plt.imshow(detached_fused_image)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefuseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DefuseModel, self).__init__()\n",
    "\n",
    "        # Downsampling\n",
    "        self.down_reflection_pad = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=1, padding=0),  \n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down_resnet_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        # Upsampling\n",
    "        self.up_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(256),  \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up_deconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up_deconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up_resnet_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.up_reflection_pad = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3), \n",
    "            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=7, stride=1, padding=0),  # 7x7 Conv with no additional padding\n",
    "            nn.InstanceNorm2d(3),  \n",
    "            nn.Tanh() \n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # print(\"Input shape : \", x.shape)\n",
    "\n",
    "        # Downsampling\n",
    "        x = self.down_reflection_pad(x)   \n",
    "        # print(\"Shape after Down 1 layer : \", x.shape)\n",
    "        x = self.down_conv1(x) \n",
    "        # print(\"Shape after Down 2 layer : \", x.shape)          \n",
    "        x = self.down_conv2(x)  \n",
    "        # print(\"Shape after Down 3 layer : \", x.shape)         \n",
    "        for i in range(1):\n",
    "            x = self.down_resnet_block1(x)  \n",
    "        for i in range(9):\n",
    "            x = self.down_resnet_block1(x)  \n",
    "        for i in range(1):\n",
    "            x = self.down_resnet_block1(x)  \n",
    "        for i in range(9):\n",
    "            x = self.down_resnet_block1(x)  \n",
    "        # print(\"Shape after Down resnet layers : \", x.shape)\n",
    "        x = self.down_conv3(x)  \n",
    "        # print(\"Shape after Down 4 layer : \", x.shape) \n",
    "        \n",
    "        \n",
    "        # Defusion\n",
    "        x1, x2 = torch.split(x, 256, dim=1)\n",
    "        # print(\"Shape after Defusion : \", x1.shape, \"and\", x2.shape)\n",
    "\n",
    "        # Upsampling\n",
    "        # x = self.up_conv1(x)\n",
    "        # print(\"Shape after Up 1 layer : \", x.shape)\n",
    "        # for i in range(1):\n",
    "        #     x = self.up_resnet_block1(x)\n",
    "        for i in range(9):\n",
    "            x1 = self.up_resnet_block1(x1)\n",
    "            x2 = self.up_resnet_block1(x2)\n",
    "        for i in range(1):\n",
    "            x1 = self.up_resnet_block1(x1)\n",
    "            x2 = self.up_resnet_block1(x2)\n",
    "        # print(\"Shape after Resnet : \", x1.shape, \"and\", x2.shape)\n",
    "        # for i in range(9):\n",
    "        #     x = self.up_resnet_block1(x)\n",
    "        # print(\"Shape after Up resnet layers : \", x.shape)\n",
    "        x1 = self.up_deconv1(x1)\n",
    "        x2 = self.up_deconv1(x2)\n",
    "        # print(\"Shape after Up 2 layer : \", x1.shape, \"and\", x2.shape)    \n",
    "        x1 = self.up_deconv2(x1)\n",
    "        x2 = self.up_deconv2(x2)\n",
    "        # print(\"Shape after Up 3 layer : \", x1.shape, \"and\", x2.shape)     \n",
    "        x1 = self.up_reflection_pad(x1)\n",
    "        x2 = self.up_reflection_pad(x2)\n",
    "        # print(\"===Output shape : \", x1.shape, \"and\", x2.shape)\n",
    "        \n",
    "        return x1, x2\n",
    "    \n",
    "# defuse_model = DefuseModel().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recovered_secret_img_1, recovered_secret_img_2 = defuse_model(fused_image)\n",
    "# detached_recovered_secret_img_1 = recovered_secret_img_1.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "# detached_recovered_secret_img_2 = recovered_secret_img_2.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "# plt.imshow(detached_recovered_secret_img_1)\n",
    "# plt.show()\n",
    "# plt.imshow(detached_recovered_secret_img_2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fused_image = fuse_model(img1, img2)\n",
    "# detached_fused_image = fused_image.squeeze().cpu().detach().numpy()\n",
    "# detached_fused_image = detached_fused_image.transpose(1, 2, 0)\n",
    "# print(detached_fused_image.shape)\n",
    "# plt.imshow(detached_fused_image)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.tensor([\n",
    "        torch.exp(torch.tensor(-(x - window_size // 2)**2 / float(2 * sigma**2))) for x in range(window_size)\n",
    "    ])\n",
    "    return gauss / gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "def ssim(img1, img2, window_size=11, channel=1, size_average=True):\n",
    "    window = create_window(window_size, channel).to(img1.device)\n",
    "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "def psnr(img1, img2):\n",
    "    mse = F.mse_loss(img1, img2)\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_fuse_model_1 = FuseModel().to(\"cuda\")\n",
    "train_defuse_model_1 = DefuseModel().to(\"cuda\")\n",
    "train_fuse_model_1_optimizer = optim.Adam(list(train_fuse_model_1.parameters()), lr = 0.01, betas = (0.5, 0.999))\n",
    "train_defuse_model_1_optimizer = optim.Adam(list(train_defuse_model_1.parameters()), lr = 0.01, betas = (0.5, 0.999))\n",
    "\n",
    "train_fuse_model_1_scaler = torch.cuda.amp.GradScaler()\n",
    "train_defuse_model_1_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "EPOCHS = 2\n",
    "lambda1 = 1.0\n",
    "lambda2 = 1.0\n",
    "lambda3 = 1.0\n",
    "\n",
    "L1_loss = nn.L1Loss()\n",
    "MSE_loss = nn.MSELoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    # reconstruction_loss_1 = torch.tensor(0.0, device=\"cuda\")\n",
    "    # reconstruction_loss_2 = torch.tensor(0.0, device=\"cuda\")\n",
    "    # ssim_loss = torch.tensor(0.0, device=\"cuda\")\n",
    "    # psnr_loss = torch.tensor(0.0, device=\"cuda\")\n",
    "    f_img = train_fuse_model_1(img1,img2)\n",
    "    df_img1, df_img2 = train_defuse_model_1(f_img)\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        reconstruction_loss_1 = (MSE_loss(img1, torch.ones_like(df_img1)) + MSE_loss(img2, torch.ones_like(df_img2))) /2\n",
    "        # reconstruction_loss_1 = (F.mse_loss(img1, df_img1) + F.mse_loss(img2, df_img2)) / 2\n",
    "        # ssim_loss += ((1-ssim(img1, df_img1)) + (1-ssim(img2, df_img2))) / 2\n",
    "        # reconstruction_loss += (-psnr(img1, df_img1) + -psnr(img2, df_img2)) / 2\n",
    "        # total_loss = lambda1*reconstruction_loss + lambda2*ssim_loss + lambda3*psnr_loss\n",
    "\n",
    "    train_fuse_model_1.train()\n",
    "    train_fuse_model_1_optimizer.zero_grad()\n",
    "    train_fuse_model_1_scaler.scale(reconstruction_loss_1).backward()\n",
    "    train_fuse_model_1_scaler.step(train_fuse_model_1_optimizer)\n",
    "    train_fuse_model_1_scaler.update()\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        reconstruction_loss_2 = (MSE_loss(img1, torch.ones_like(df_img1)) + MSE_loss(img2, torch.ones_like(df_img2))) /2\n",
    "    train_defuse_model_1.train()\n",
    "    train_defuse_model_1_optimizer.zero_grad()\n",
    "    train_defuse_model_1_scaler.scale(reconstruction_loss_2).backward()\n",
    "    train_defuse_model_1_scaler.step(train_defuse_model_1_optimizer)\n",
    "    train_defuse_model_1_scaler.update()\n",
    "\n",
    "    del f_img, df_img1, df_img2, reconstruction_loss_1, reconstruction_loss_2\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_fuse_model_1 = FuseModel().to(\"cuda\")\n",
    "train_defuse_model_1 = DefuseModel().to(\"cuda\")\n",
    "train_fuse_model_1_optimizer = optim.Adam(list(train_fuse_model_1.parameters()), lr = 0.01, betas = (0.5, 0.999))\n",
    "train_defuse_model_1_optimizer = optim.Adam(list(train_defuse_model_1.parameters()), lr = 0.01, betas = (0.5, 0.999))\n",
    "\n",
    "train_fuse_model_1_scaler = torch.cuda.amp.GradScaler()\n",
    "train_defuse_model_1_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "EPOCHS = 2\n",
    "lambda1 = 1.0\n",
    "lambda2 = 1.0\n",
    "lambda3 = 1.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    # reconstruction_loss_1 = torch.tensor(0.0, device=\"cuda\")\n",
    "    # reconstruction_loss_2 = torch.tensor(0.0, device=\"cuda\")\n",
    "    # ssim_loss = torch.tensor(0.0, device=\"cuda\")\n",
    "    # psnr_loss = torch.tensor(0.0, device=\"cuda\")\n",
    "    f_img = train_fuse_model_1(img1,img2)\n",
    "    df_img1, df_img2 = train_defuse_model_1(f_img)\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        reconstruction_loss_1 = (F.mse_loss(img1, df_img1) + F.mse_loss(img2, df_img2)) / 2\n",
    "        # ssim_loss += ((1-ssim(img1, df_img1)) + (1-ssim(img2, df_img2))) / 2\n",
    "        # reconstruction_loss += (-psnr(img1, df_img1) + -psnr(img2, df_img2)) / 2\n",
    "        # total_loss = lambda1*reconstruction_loss + lambda2*ssim_loss + lambda3*psnr_loss\n",
    "\n",
    "    train_fuse_model_1.train()\n",
    "    train_fuse_model_1_optimizer.zero_grad()\n",
    "    train_fuse_model_1_scaler.scale(reconstruction_loss_1).backward(retain_graph=True)\n",
    "    train_fuse_model_1_scaler.step(train_fuse_model_1_optimizer)\n",
    "    train_fuse_model_1_scaler.update()\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        reconstruction_loss_2 = (F.mse_loss(img1, df_img1) + F.mse_loss(img2, df_img2)) / 2\n",
    "    train_defuse_model_1.train()\n",
    "    train_defuse_model_1_optimizer.zero_grad()\n",
    "    train_defuse_model_1_scaler.scale(reconstruction_loss_2).backward()\n",
    "    train_defuse_model_1_scaler.step(train_defuse_model_1_optimizer)\n",
    "    train_defuse_model_1_scaler.update()\n",
    "\n",
    "    del f_img, df_img1, df_img2, reconstruction_loss_1, reconstruction_loss_2\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
